{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Prediction & Notification Uplift Model\n",
    "**Predicting Churn Timing and Measuring Notification Impact for Long-Tenured Wellness Users**\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements a two-model approach following the **CRISP-DM** methodology:\n",
    "\n",
    "1. **Churn Prediction Model** \u2014 An XGBoost classifier that detects behavioral drift (declining sessions, growing inactivity gaps, reduced workout duration) to identify users likely to cancel within 30 days.\n",
    "2. **Uplift Analysis** \u2014 Measures the causal impact of push notifications on churn reduction across user segments using real notification data.\n",
    "\n",
    "### Key Design Decisions\n",
    "- **Long-tenured users only** \u2014 All users have 180+ days of tenure. The model focuses on retention bleed, not early-stage drop-offs.\n",
    "- **Drift-based features** \u2014 Rather than static snapshots, the model compares a user's *baseline* activity (45\u201390 days before decision) against their *recent* activity (last 30 days) to detect disengagement trajectories.\n",
    "- **Temporal integrity** \u2014 All features are computed from activity strictly before the decision date to prevent data leakage.\n",
    "- **Separated concerns** \u2014 The churn prediction model and the uplift analysis are kept independent; the notification flag is NOT used as a model feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & Data Loading\n",
    "\n",
    "The dataset consists of six tables covering user demographics, subscription details, workout sessions, support contacts, notification logs, and cancellation events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, classification_report,\n",
    "    confusion_matrix, precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "DATA_PATH = 'data/agile_churn_raw_v11.xlsx'\n",
    "\n",
    "print(f'Loading data from: {DATA_PATH}...')\n",
    "df_users    = pd.read_excel(DATA_PATH, sheet_name='users_raw')\n",
    "df_subs     = pd.read_excel(DATA_PATH, sheet_name='subscriptions_raw')\n",
    "df_cancel   = pd.read_excel(DATA_PATH, sheet_name='cancellations_raw')\n",
    "df_notif    = pd.read_excel(DATA_PATH, sheet_name='notifications_raw')\n",
    "df_sessions = pd.read_excel(DATA_PATH, sheet_name='sessions_raw')\n",
    "df_support  = pd.read_excel(DATA_PATH, sheet_name='support_raw')\n",
    "\n",
    "print(f'Loaded \u2014 {len(df_users):,} users | {len(df_sessions):,} sessions | '\n",
    "      f'{len(df_cancel):,} cancellations | {len(df_notif):,} notifications')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning & Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users['decision_date']  = pd.to_datetime(df_users['decision_date'])\n",
    "df_subs['first_paid_date'] = pd.to_datetime(df_subs['first_paid_date'])\n",
    "df_cancel['churn_ts']      = pd.to_datetime(df_cancel['churn_ts'])\n",
    "df_sessions['start_ts']    = pd.to_datetime(df_sessions['start_ts'])\n",
    "df_notif['sent_ts']        = pd.to_datetime(df_notif['sent_ts'])\n",
    "df_support['contact_ts']   = pd.to_datetime(df_support['contact_ts'])\n",
    "\n",
    "print('Date conversion complete.')\n",
    "print(f'User decision dates: {df_users[\"decision_date\"].min().date()} to {df_users[\"decision_date\"].max().date()}')\n",
    "print(f'Session dates: {df_sessions[\"start_ts\"].min().date()} to {df_sessions[\"start_ts\"].max().date()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Population & Churn Definition\n",
    "\n",
    "All users in this dataset are long-tenured (180+ day tenure), which aligns with the project scope of preventing retention bleed among established subscribers.\n",
    "\n",
    "**Churn definition:** A user is labeled as churned if their cancellation event falls within a 30-day forward window from their decision date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge base tables\n",
    "model_df = df_users.merge(df_subs, on='user_id', how='left')\n",
    "model_df = model_df.merge(df_cancel[['user_id', 'churn_ts']], on='user_id', how='left')\n",
    "\n",
    "# Tenure\n",
    "model_df['tenure_days'] = (model_df['decision_date'] - model_df['first_paid_date']).dt.days\n",
    "print(f'Population: {len(model_df):,} long-tenured users')\n",
    "print(f'Tenure range: {model_df[\"tenure_days\"].min()} \u2013 {model_df[\"tenure_days\"].max()} days '\n",
    "      f'(mean: {model_df[\"tenure_days\"].mean():.0f})')\n",
    "\n",
    "# Churn label (30-day forward window)\n",
    "CHURN_WINDOW = 30\n",
    "churn_mask = (\n",
    "    (model_df['churn_ts'].notna()) &\n",
    "    (model_df['churn_ts'] >= model_df['decision_date']) &\n",
    "    (model_df['churn_ts'] <= model_df['decision_date'] + pd.Timedelta(days=CHURN_WINDOW))\n",
    ")\n",
    "model_df['target_churn'] = churn_mask.astype(int)\n",
    "\n",
    "churn_rate = model_df['target_churn'].mean()\n",
    "print(f'\\nChurn rate ({CHURN_WINDOW}-day window): {churn_rate:.1%}')\n",
    "print(f'  Churners: {model_df[\"target_churn\"].sum():,}  |  Retained: {(model_df[\"target_churn\"]==0).sum():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Drift-Based Feature Engineering\n",
    "\n",
    "This is the core of the modeling approach. Instead of using static, point-in-time metrics, we compute **behavioral drift** \u2014 the change between a user's established baseline and their recent activity.\n",
    "\n",
    "### Time Windows\n",
    "| Window | Period | Purpose |\n",
    "|--------|--------|---------|\n",
    "| **Baseline** | 45\u201390 days before decision | Established behavioral patterns |\n",
    "| **Recent** | 0\u201330 days before decision | Current engagement state |\n",
    "\n",
    "### Drift Features\n",
    "| Feature | Formula | Signal |\n",
    "|---------|---------|--------|\n",
    "| `session_decline_rate` | (recent_count \u2212 base_count) / base_count | Frequency decay |\n",
    "| `duration_drift` | recent_avg_duration \u2212 base_avg_duration | Engagement quality drop |\n",
    "| `inactivity_gap_growth` | recent_avg_gap \u2212 base_avg_gap | Growing gaps between sessions |\n",
    "| `calorie_trend` | (recent_calories \u2212 base_calories) / base_calories | Effort decline |\n",
    "| `days_since_last_workout` | Days from last session to decision date | Immediate recency signal |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Merge sessions with decision dates \u2500\u2500\n",
    "sess = df_sessions.merge(df_users[['user_id', 'decision_date']], on='user_id', how='left')\n",
    "sess['days_before'] = (sess['decision_date'] - sess['start_ts']).dt.days\n",
    "sess_pre = sess[sess['days_before'] >= 0]  # only pre-decision sessions\n",
    "\n",
    "# \u2500\u2500 Define time windows \u2500\u2500\n",
    "baseline_window = sess_pre[(sess_pre['days_before'] >= 45) & (sess_pre['days_before'] <= 90)]\n",
    "recent_window   = sess_pre[sess_pre['days_before'] <= 30]\n",
    "\n",
    "print(f'Baseline window (45-90 days): {len(baseline_window):,} sessions across {baseline_window[\"user_id\"].nunique():,} users')\n",
    "print(f'Recent window (0-30 days):    {len(recent_window):,} sessions across {recent_window[\"user_id\"].nunique():,} users')\n",
    "\n",
    "def window_agg(df, prefix):\n",
    "    \"\"\"Aggregate session metrics within a time window.\"\"\"\n",
    "    agg = df.groupby('user_id').agg(\n",
    "        count=('session_id', 'count'),\n",
    "        avg_dur=('duration_min', 'mean'),\n",
    "        total_cal=('calories', 'sum'),\n",
    "    ).reset_index()\n",
    "    # Average gap between consecutive sessions\n",
    "    gaps = (df.sort_values(['user_id', 'start_ts'])\n",
    "            .groupby('user_id')['start_ts']\n",
    "            .apply(lambda x: x.diff().dt.days.mean() if len(x) > 1 else np.nan)\n",
    "            .reset_index(name='avg_gap'))\n",
    "    agg = agg.merge(gaps, on='user_id', how='left')\n",
    "    return agg.rename(columns={c: f'{prefix}_{c}' for c in agg.columns if c != 'user_id'})\n",
    "\n",
    "base_features = window_agg(baseline_window, 'base')\n",
    "recent_features = window_agg(recent_window, 'recent')\n",
    "\n",
    "# \u2500\u2500 Compute drift metrics \u2500\u2500\n",
    "drift_df = (df_users[['user_id']]\n",
    "            .merge(base_features, on='user_id', how='left')\n",
    "            .merge(recent_features, on='user_id', how='left'))\n",
    "\n",
    "# Fill missing windows (no sessions in that period)\n",
    "drift_df = drift_df.fillna({\n",
    "    'base_count': 0, 'recent_count': 0,\n",
    "    'base_avg_dur': 0, 'recent_avg_dur': 0,\n",
    "    'base_total_cal': 0, 'recent_total_cal': 0,\n",
    "    'base_avg_gap': 999, 'recent_avg_gap': 999\n",
    "})\n",
    "\n",
    "# Session decline rate\n",
    "drift_df['session_decline_rate'] = np.where(\n",
    "    drift_df['base_count'] > 0,\n",
    "    (drift_df['recent_count'] - drift_df['base_count']) / drift_df['base_count'],\n",
    "    np.where(drift_df['recent_count'] > 0, 0, -1)\n",
    ")\n",
    "\n",
    "# Duration drift (change in avg workout length)\n",
    "drift_df['duration_drift'] = drift_df['recent_avg_dur'] - drift_df['base_avg_dur']\n",
    "\n",
    "# Inactivity gap growth (widening gaps = disengagement)\n",
    "drift_df['inactivity_gap_growth'] = drift_df['recent_avg_gap'] - drift_df['base_avg_gap']\n",
    "\n",
    "# Calorie trend\n",
    "drift_df['calorie_trend'] = np.where(\n",
    "    drift_df['base_total_cal'] > 0,\n",
    "    (drift_df['recent_total_cal'] - drift_df['base_total_cal']) / drift_df['base_total_cal'],\n",
    "    0\n",
    ")\n",
    "\n",
    "print('\\nDrift features computed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Overall session metrics & recency \u2500\u2500\n",
    "total_stats = sess_pre.groupby('user_id').agg(\n",
    "    total_sessions=('session_id', 'count'),\n",
    "    avg_duration=('duration_min', 'mean'),\n",
    "    total_calories=('calories', 'sum'),\n",
    "    days_since_last_workout=('days_before', 'min'),\n",
    "    workout_type_mode=('workout_type', lambda x: x.mode()[0] if not x.mode().empty else 'Other')\n",
    ").reset_index()\n",
    "\n",
    "# \u2500\u2500 Support tickets \u2500\u2500\n",
    "support_counts = df_support.groupby('user_id').size().reset_index(name='support_tickets')\n",
    "\n",
    "# \u2500\u2500 Merge all features \u2500\u2500\n",
    "model_df = model_df.merge(drift_df, on='user_id', how='left')\n",
    "model_df = model_df.merge(total_stats, on='user_id', how='left')\n",
    "model_df = model_df.merge(support_counts, on='user_id', how='left')\n",
    "\n",
    "# \u2500\u2500 Imputation \u2500\u2500\n",
    "model_df['total_sessions'] = model_df['total_sessions'].fillna(0)\n",
    "model_df['avg_duration'] = model_df['avg_duration'].fillna(0)\n",
    "model_df['total_calories'] = model_df['total_calories'].fillna(0)\n",
    "model_df['days_since_last_workout'] = model_df['days_since_last_workout'].fillna(999)\n",
    "model_df['support_tickets'] = model_df['support_tickets'].fillna(0)\n",
    "model_df['workout_type_mode'] = model_df['workout_type_mode'].fillna('None')\n",
    "\n",
    "# \u2500\u2500 Categorical encodings \u2500\u2500\n",
    "model_df['gender_code'] = model_df['gender'].astype('category').cat.codes\n",
    "model_df['billing_code'] = model_df['billing_cycle'].astype('category').cat.codes\n",
    "model_df['activity_code'] = model_df['workout_type_mode'].astype('category').cat.codes\n",
    "\n",
    "# \u2500\u2500 Activity cohorts (tercile buckets) \u2500\u2500\n",
    "model_df['activity_bucket'] = pd.qcut(\n",
    "    model_df['total_sessions'], q=3,\n",
    "    labels=['Low Activity', 'Medium Activity', 'High Activity']\n",
    ")\n",
    "\n",
    "# \u2500\u2500 Notification flag (from REAL notification data) \u2500\u2500\n",
    "model_df['received_notification'] = model_df['user_id'].isin(df_notif['user_id'].unique()).astype(int)\n",
    "\n",
    "print(f'Feature engineering complete \u2014 {model_df.shape[1]} columns.')\n",
    "print(f'\\nActivity cohort distribution:')\n",
    "print(model_df['activity_bucket'].value_counts().sort_index())\n",
    "print(f'\\nNotification distribution: {model_df[\"received_notification\"].mean():.1%} received')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "### 5a. Drift Features vs Churn\n",
    "If the drift features capture real disengagement signals, churners should show declining sessions, shrinking durations, and growing inactivity gaps compared to retained users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift features: churned vs retained\n",
    "drift_cols = ['session_decline_rate', 'duration_drift', 'inactivity_gap_growth',\n",
    "              'days_since_last_workout', 'recent_count', 'base_count']\n",
    "\n",
    "print('Drift Features \u2014 Churned vs Retained (mean):')\n",
    "print('-' * 75)\n",
    "for col in drift_cols:\n",
    "    churned = model_df[model_df['target_churn'] == 1][col].mean()\n",
    "    retained = model_df[model_df['target_churn'] == 0][col].mean()\n",
    "    print(f'  {col:30s}  Churned: {churned:+9.2f}  |  Retained: {retained:+9.2f}  |  \u0394: {churned - retained:+9.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn rate by activity cohort\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sns.barplot(x='activity_bucket', y='target_churn', data=model_df,\n",
    "            palette='viridis', ci=None, ax=axes[0])\n",
    "axes[0].set_title('Churn Rate by Activity Cohort')\n",
    "axes[0].set_ylabel('Churn Probability')\n",
    "\n",
    "# Drift distributions by churn status\n",
    "sns.boxplot(x='target_churn', y='days_since_last_workout', data=model_df,\n",
    "            palette='Set2', ax=axes[1])\n",
    "axes[1].set_title('Recency: Days Since Last Workout')\n",
    "axes[1].set_xlabel('Churned')\n",
    "\n",
    "sns.boxplot(x='target_churn', y='duration_drift', data=model_df,\n",
    "            palette='Set2', ax=axes[2])\n",
    "axes[2].set_title('Duration Drift (Recent \u2212 Baseline)')\n",
    "axes[2].set_xlabel('Churned')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional EDA: support tickets and session decline\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.boxplot(x='target_churn', y='support_tickets', data=model_df,\n",
    "            palette='Set2', ax=axes[0])\n",
    "axes[0].set_title('Friction: Support Tickets')\n",
    "axes[0].set_xlabel('Churned')\n",
    "\n",
    "sns.boxplot(x='target_churn', y='session_decline_rate', data=model_df,\n",
    "            palette='Set2', ax=axes[1])\n",
    "axes[1].set_title('Session Decline Rate (Recent vs Baseline)')\n",
    "axes[1].set_xlabel('Churned')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Churn Prediction Model (XGBoost)\n",
    "\n",
    "The model is trained on drift-based features, overall activity metrics, demographics, and friction signals. The `received_notification` flag is deliberately **excluded** \u2014 the churn model predicts risk independent of interventions, while notification impact is measured separately in the uplift analysis.\n",
    "\n",
    "We validate with **5-fold stratified cross-validation** to confirm the model generalizes, then train a final model on an 80/20 holdout for detailed evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Feature set (no treatment indicator) \u2500\u2500\n",
    "FEATURES = [\n",
    "    # Drift features (core signal)\n",
    "    'session_decline_rate', 'duration_drift',\n",
    "    'inactivity_gap_growth', 'calorie_trend',\n",
    "    'recent_count', 'base_count',\n",
    "    # Overall metrics\n",
    "    'total_sessions', 'avg_duration', 'days_since_last_workout',\n",
    "    # Demographics & friction\n",
    "    'age', 'height_cm', 'weight_kg', 'tenure_days',\n",
    "    'support_tickets', 'gender_code', 'billing_code'\n",
    "]\n",
    "\n",
    "X = model_df[FEATURES].fillna(0)\n",
    "y = model_df['target_churn']\n",
    "\n",
    "# \u2500\u2500 Cross-validation \u2500\u2500\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(xgb_model, X, y, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(f'5-Fold Stratified CV ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})')\n",
    "print(f'Per-fold scores: {[f\"{s:.4f}\" for s in cv_scores]}')\n",
    "\n",
    "# \u2500\u2500 Final holdout model \u2500\u2500\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f'\\nTraining on {len(X_train):,} samples, evaluating on {len(X_test):,}...')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "For imbalanced churn data, ROC-AUC alone is insufficient. We also evaluate with:\n",
    "- **Precision-Recall curve** \u2014 Measures how well the model captures true churners without excessive false alarms\n",
    "- **Youden's J statistic** \u2014 Finds the optimal classification threshold\n",
    "- **Classification report** \u2014 Precision, recall, and F1 at the optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# \u2500\u2500 ROC Curve \u2500\u2500\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "J = tpr - fpr\n",
    "ix = np.argmax(J)\n",
    "best_thresh = thresholds[ix]\n",
    "auc_score = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].plot(fpr, tpr, label=f'XGBoost (AUC = {auc_score:.3f})', color='purple', lw=2)\n",
    "axes[0].scatter(fpr[ix], tpr[ix], marker='o', color='black',\n",
    "                label=f'Optimal Threshold = {best_thresh:.2f}', zorder=5)\n",
    "axes[0].plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend()\n",
    "\n",
    "# \u2500\u2500 Precision-Recall Curve \u2500\u2500\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "ap_score = average_precision_score(y_test, y_probs)\n",
    "\n",
    "axes[1].plot(recall, precision, label=f'AP = {ap_score:.2f}', color='teal', lw=2)\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# \u2500\u2500 Classification Report \u2500\u2500\n",
    "y_pred = (y_probs >= best_thresh).astype(int)\n",
    "print(f'Classification Report (threshold = {best_thresh:.2f}):\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Explainability (SHAP)\n",
    "\n",
    "SHAP values provide transparency into which features drive each prediction. If the model is sound, drift-based features should dominate the importance rankings \u2014 confirming the model relies on behavioral change signals rather than static demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Bar chart (global importance)\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test, plot_type='bar', show=False)\n",
    "plt.title('Feature Importance (SHAP)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Beeswarm (direction of impact)\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test, show=False)\n",
    "plt.title('Feature Impact Direction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Uplift Analysis \u2014 Notification Impact\n",
    "\n",
    "This section measures the **causal effect** of push notifications on churn using the real notification data. Unlike the predictive model (which identifies *who* is at risk), the uplift analysis answers *what works* to reduce churn.\n",
    "\n",
    "The treatment group consists of users who actually received notifications (`notifications_raw`), and the control group is the remaining users.\n",
    "\n",
    "**Uplift = Churn Rate (Treatment) \u2212 Churn Rate (Control)**\n",
    "- A negative value indicates notifications successfully reduced churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 9a. Overall Notification Uplift \u2500\u2500\n",
    "print('=' * 60)\n",
    "print('  NOTIFICATION UPLIFT ANALYSIS (Real Data)')\n",
    "print('=' * 60)\n",
    "\n",
    "uplift_overall = model_df.groupby('received_notification')['target_churn'].agg(['mean', 'count'])\n",
    "uplift_overall.index = ['Control (No Notification)', 'Treatment (Notified)']\n",
    "uplift_overall.columns = ['Churn Rate', 'Users']\n",
    "print(uplift_overall)\n",
    "\n",
    "control_rate = uplift_overall.loc['Control (No Notification)', 'Churn Rate']\n",
    "treatment_rate = uplift_overall.loc['Treatment (Notified)', 'Churn Rate']\n",
    "print(f'\\nOverall Uplift: {(treatment_rate - control_rate)*100:+.2f} percentage points')\n",
    "print(f'Relative reduction: {(treatment_rate - control_rate)/control_rate*100:+.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 9b. Uplift by Activity Cohort \u2500\u2500\n",
    "print('\\n' + '=' * 60)\n",
    "print('  UPLIFT BY ACTIVITY COHORT')\n",
    "print('=' * 60)\n",
    "\n",
    "cohort_uplift = model_df.groupby(\n",
    "    ['activity_bucket', 'received_notification']\n",
    ")['target_churn'].mean().unstack()\n",
    "\n",
    "cohort_uplift.columns = ['Control', 'Treatment']\n",
    "cohort_uplift['Uplift_PP'] = (cohort_uplift['Treatment'] - cohort_uplift['Control']) * 100\n",
    "print(cohort_uplift)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "sns.barplot(x='activity_bucket', y='target_churn', hue='received_notification',\n",
    "            data=model_df, palette='coolwarm', ax=axes[0])\n",
    "axes[0].set_title('Notification Impact by Activity Cohort')\n",
    "axes[0].set_ylabel('Churn Rate')\n",
    "axes[0].legend(title='Notified', labels=['No', 'Yes'])\n",
    "\n",
    "# By workout type\n",
    "type_uplift = model_df.groupby(\n",
    "    ['workout_type_mode', 'received_notification']\n",
    ")['target_churn'].mean().unstack()\n",
    "type_uplift.columns = ['Control', 'Treatment']\n",
    "type_uplift['Uplift_PP'] = (type_uplift['Treatment'] - type_uplift['Control']) * 100\n",
    "\n",
    "sns.barplot(x='workout_type_mode', y='target_churn', hue='received_notification',\n",
    "            data=model_df, palette='magma', ax=axes[1])\n",
    "axes[1].set_title('Notification Impact by Workout Type')\n",
    "axes[1].set_ylabel('Churn Rate')\n",
    "axes[1].legend(title='Notified', labels=['No', 'Yes'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nUplift by Workout Type:')\n",
    "print(type_uplift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Appendix \u2014 Data Distributions & Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.histplot(model_df['age'], bins=15, kde=True, ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Age Distribution')\n",
    "\n",
    "sns.countplot(x='gender', data=model_df, ax=axes[1], palette='pastel')\n",
    "axes[1].set_title('Gender Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_cols = ['target_churn', 'days_since_last_workout', 'session_decline_rate',\n",
    "             'duration_drift', 'total_sessions', 'support_tickets', 'age']\n",
    "sns.heatmap(model_df[corr_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Automated Business Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('  BUSINESS INSIGHTS SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "# 1. Baseline\n",
    "print(f'1. BASELINE: Overall churn rate is {model_df[\"target_churn\"].mean():.1%}.')\n",
    "\n",
    "# 2. Risk signal\n",
    "median_recency = model_df[model_df['target_churn']==1]['days_since_last_workout'].median()\n",
    "print(f'2. RISK SIGNAL: Churned users stop working out ~{median_recency:.0f} days before cancelling.')\n",
    "\n",
    "# 3. Drift insight\n",
    "churned_decline = model_df[model_df['target_churn']==1]['session_decline_rate'].mean()\n",
    "retained_decline = model_df[model_df['target_churn']==0]['session_decline_rate'].mean()\n",
    "print(f'3. DRIFT: Churners show session decline of {churned_decline:+.2f} vs {retained_decline:+.2f} for retained.')\n",
    "\n",
    "# 4. Cohorts\n",
    "best_cohort = cohort_uplift['Control'].idxmin()\n",
    "worst_cohort = cohort_uplift['Control'].idxmax()\n",
    "print(f'4. COHORTS: \\'{best_cohort}\\' users are safest. \\'{worst_cohort}\\' users are most at risk.')\n",
    "\n",
    "# 5. Notification strategy\n",
    "overall_uplift = (treatment_rate - control_rate) * 100\n",
    "print(f'5. NOTIFICATIONS: {overall_uplift:+.2f} pp churn reduction for notified users.')\n",
    "\n",
    "# 6. Model performance\n",
    "print(f'6. MODEL: ROC-AUC = {auc_score:.3f} | CV-AUC = {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})')\n",
    "\n",
    "print('=' * 60)"
   ]
  }
 ]
}